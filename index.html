<!DOCTYPE html>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async="" src="./files/js"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-WYV42QGLZ8');
  </script>

  <meta name="author" content="Peng Gao">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Peng Gao's Homepage.">
  <title>gaopeng</title>

  <link rel="stylesheet" href="./files/font.css">
  <link rel="stylesheet" href="./files/main.css">
  <script src="./files/main.js"></script>
  <script src="./files/scroll.js"></script>
  <link rel="stylesheet" href="./files/button.css">
  <script src="./files/button.js"></script>
</head>

<body data-new-gr-c-s-check-loaded="14.993.0" data-gr-ext-installed="">
    <div class="outercontainer"> 
      <header>
        <div class="container header">
        </div>
      </header>
      <div class="container body">
        <div class="content heading anchor" id="home" data-scroll-id="home" tabindex="-1" style="outline: none;">
          <div class="text info">
            <h1>Peng Gao</h1>
            <p>
            </p>
            <div>Young Scientist</div>
            <div>Shanghai AI Lab</div>
            <div>Email:&nbsp;gaopeng [at] pjlab (dot) org (dot) cn</div>
            <p>
            <span><a href="https://scholar.google.com/citations?user=miFIAFMAAAAJ&hl=en">Google Scholar</a></span> / 
            <span><a href="https://github.com/gaopengcuhk">Github</a></span>
            </p>
            <p>
            </p>
          </div>
          <div class="img"><img class="avatar" src="./imgs/me.jpg" alt="Photo"></div>
          
          <div class="text info">
            <p>I am a Young Scientist at Shanghai AI Lab. I got my Ph.D. degree from Multimedia Lab, the Chinese University of Hong Kong in 2021. During my Ph.D. period, I was supervised by Xiaogang Wang and Hongsheng Li. I was luckily to be involved in internship program at MERL Boston, Microsoft Seattle, AI2 Seattle and Sensetime Beijing/Shenzhen during my Ph.D. time. My research interestes lie in multi-modality Learning, efficient visual backbone design, self-supervised representation learning. 
               <br><br>
               If you are interested in research intern, research engineer, full-time researcher at Shanghai AI lab or Ph.D. program of MMLAB at CUHK. Please send me an email. 
            </p>
          </div>
        </div>
        <div class="img"><img class="pipe" src="./imgs/pipeline.jpg" alt="Pipe"></div>
        <div class="content" style="z-index:1;position:relative">
          <div class="text">
            <h2 style="margin-bottom:.5em">News</h2>
            <ul style="padding-bottom:1em">
              <li><strong>[10/2021]</strong> Five paper are accepted by <a href="https://eccv2022.ecva.net/" target="_blank">ECCV 2022</a>.</li>
              <li><strong>[05/2022]</strong> Vision team at Shanghai AI Lab realeased <a href="https://github.com/Alpha-VL/ConvMAE/" target="_blank">ConvMAE, FastConvMAE and VideoConvMAE</a>.</li>
              <li><strong>[01/2022]</strong> PointCLIP accepted by <a href="https://cvpr2022.thecvf.com/" target="_blank">CVPR 2022</a>.</li>
              <li><strong>[01/2022]</strong> A strong image and video Backbone Uniformer accepted by <a href="https://iclr.cc/" target="_blank">ICLR2022</a>.</li>
              <li><strong>[11/2021]</strong> Vision team at Shanghai AI Lab realeased Tip-Adaptor on <a href="https://arxiv.org/" target="_blank">Arxiv</a>.</li>
              <li><strong>[10/2021]</strong> Vision team at Shanghai AI Lab realeased CLIP-Adaptor on <a href="https://arxiv.org/" target="_blank">Arxiv</a>.</li>
              <li><strong>[10/2021]</strong> Attempts to replicate interesting paper Pix2Seq is released at <a href="https://github.com/gaopengcuhk/Unofficial-Pix2Seq" target="_blank">Unoffical Pix2Seq</a>.</li>
              <li><strong>[10/2021]</strong> Two paper are accepted by <a href="https://nips.cc/Conferences/2021" target="_blank">NeuIPS 2021</a>.</li>
              <li><strong>[07/2021]</strong> One paper is accepted by <a href="http://iccv2021.thecvf.com/home/" target="_blank">ICCV 2021</a>.</li>
              <li><strong>[06/2021]</strong> One paper is accepted by <a href="https://2021.acmmm.org/" target="_blank">ACMMM 2021</a>.</li>
              <li><strong>[02/2021]</strong> One paper is accepted by <a href="https://aaai.org/Conferences/AAAI-21/" target="_blank">AAAI 2021</a>.</li>
            </ul>
          </div>
        </div>


        <div class="content anchor" id="publications">
          <div class="text" style="z-index:1;position:relative">
            <h2 style="margin-bottom:0em">
              Publications
            </h2>
          </div>
          

          <div id="pubs">
            <div class="text anchor">&nbsp;</div>
            <div class="year" id="year"></div>
            <div class="text">
              <h3 style="margin-bottom:0em">
                2022
              </h3>


            </div>
             <div class="publication">
              <div class="img"><img class="img_responsive" src="./imgs/point_m2ae.png" alt="autoassign"></div>
              <div class="text">
                <div class="title">ConvMAE: Masked Convolution Meets Masked Autoencoders</div>
                <div class="authors">
                  <span class="author jw">Peng Gao</span>,
                  <span class="author">Teli Ma</span>,
                  <span class="author">Hongsheng Li</span>,
                  <span class="author">Ziyi Lin</span>,
                  <span class="author">Jifeng Dai</span>,
                  <span class="author">Yu Qiao</span>,
                </div>
                <div>
                  <span class="venue">Arxiv 2022</span> /
                  <span class="tag"><a href="https://arxiv.org/pdf/2205.03892.pdf">Paper</a></span> /
                  <span class="tag"><a href="https://github.com/Alpha-VL/ConvMAE">Code</a></span>
                </div>
                <br>
              </div>
            </div>   
              
              
            </div>
             <div class="publication">
              <div class="img"><img class="img_responsive" src="./imgs/point_m2ae.png" alt="autoassign"></div>
              <div class="text">
                <div class="title">Point-M2AE: Multi-scale Masked Autoencoders for Hierarchical Point Cloud Pre-training</div>
                <div class="authors">
                  <span class="author">Renrui Zhang</span>,
                  <span class="author">Ziyu Guo</span>,
                  <span class="author jw">Peng Gao</span>,
                  <span class="author">Rongyao Fang</span>,
                  <span class="author">Bin Zhao</span>,
                  <span class="author">Dong Wang</span>,
                  <span class="author">Yu Qiao</span>,
                  <span class="author">Hongsheng Li</span>,
                </div>
                <div>
                  <span class="venue">Arxiv 2022</span> /
                  <span class="tag"><a href="https://arxiv.org/pdf/2205.03892.pdf">Paper</a></span> /
                  <span class="tag"><a href="https://github.com/ZrrSkywalker/Point-M2AE">Code</a></span>
                </div>
                <br>
              </div>
            </div>   
              
              
            </div>
             <div class="publication">
              <div class="img"><img class="img_responsive" src="./imgs/pointclip.png" alt="autoassign"></div>
              <div class="text">
                <div class="title">PointCLIP: Point Cloud Understanding by CLIP</div>
                <div class="authors">
                  <span class="author">Renrui Zhang*</span>,
                  <span class="author">Ziyu Guo*</span>,
                  <span class="author">Wei Zhang</span>,
                  <span class="author">Kunchang Li</span>,
                  <span class="author">Xupeng Miao</span>,
                  <span class="author">Bin Cui</span>,
                  <span class="author">Yu Qiao</span>,
                  <span class="author jw">Peng Gao**</span>,
                  <span class="author">Hongsheng Li</span>,
                </div>
                <div>
                  <span class="venue">CVPR 2022</span> /
                  <span class="tag"><a href="https://arxiv.org/pdf/2112.02413.pdf">Paper</a></span> /
                  <span class="tag"><a href="https://github.com/ZrrSkywalker/PointCLIP">Code</a></span>
                </div>
                <br>
              </div>
            </div>    
              

            </div>
             <div class="publication">
              <div class="img"><img class="img_responsive" src="./imgs/uniformer.png" alt="autoassign"></div>
              <div class="text">
                <div class="title">UniFormer: Unified Transformer for Efficient Spatiotemporal Representation Learning </div>
                <div class="authors">
                  <span class="author">Kunchang Li</span>,
                  <span class="author">Yali Wang</span>,
                  <span class="author jw">Peng Gao</span>,
                  <span class="author">Guanglu Song</span>,
                  <span class="author">Yu Liu</span>,
                  <span class="author">Hongsheng Li</span>,
                  <span class="author">Yu Qiao</span>,
                </div>
                <div>
                  <span class="venue">ICLR 2022</span> /
                  <span class="tag"><a href="https://arxiv.org/abs/2201.04676.pdf">Paper</a></span> /
                  <span class="tag"><a href="https://github.com/Sense-X/UniFormer">Code</a></span>
                </div>
                <br>
              </div>
            </div>  
            

            <div class="year" id="year"></div>
            <div class="text">
              <h3 style="margin-bottom:0em">
                2021
              </h3>
            </div>

             <div class="publication">
              <div class="img"><img class="img_responsive" src="./imgs/ballad.png" alt="autoassign"></div>
              <div class="text">
                <div class="title">A Simple Long-Tailed Recognition Baseline via Vision-Language Model</div>
                <div class="authors">
                  <span class="author">Teli Ma*</span>,
                  <span class="author"> Shijie Geng</span>,
                  <span class="author"> Mengmeng Wang</span>,
                  <span class="author"> Jing Shao</span>,
                  <span class="author">Jiasen Lu</span>,
                  <span class="author">Hongsheng Li</span>,
                  <span class="author jw">Peng Gao**</span>,
                  <span class="author">Yu Qiao</span>,
                </div>
                <div>
                  <span class="venue">Arxiv 2021</span> /
                  <span class="tag"><a href="https://arxiv.org/pdf/2111.14745.pdf">Paper</a></span> /
                  <span class="tag"><a href="https://github.com/gaopengcuhk/BALLAD">Code</a></span>
                </div>
                <br>
              </div>
            </div>  
            

            
           
            
            <div class="publication">
              <div class="img"><img class="img_responsive" src="./imgs/clip_adaptor.jpg" alt="autoassign"></div>
              <div class="text">
                <div class="title">CLIP-Adapter: Better Vision-Language Models with Feature Adapters</div>
                <div class="authors">
                  <span class="author jw">Peng Gao*</span>,
                  <span class="author">Shijie Geng*</span>,
                  <span class="author">Renrui Zhang*</span>,
                  <span class="author">Teli Ma</span>,
                  <span class="author">Rongyao Fang</span>,
                  <span class="author">Yongfeng Zhang</span>,
                  <span class="author">Hongsheng Li</span>,
                  <span class="author">Yu Qiao</span>,
                </div>
                <div>
                  <span class="venue">Arxiv 2021</span> /
                  <span class="tag"><a href="https://github.com/gaopengcuhk/CLIP-Adapter/blob/main/Adapter.pdf">Paper</a></span> /
                  <span class="tag"><a href="https://github.com/gaopengcuhk/CLIP-Adapter">Code</a></span>
                </div>
                <br>
              </div>
            </div>   


            <div class="publication">
              <div class="img"><img class="img_responsive" src="./imgs/container.jpg" alt="autoassign"></div>
              <div class="text">
                <div class="title">Container : Context Aggregation Network</div>
                <div class="authors">
                  <span class="author jw">Peng Gao*</span>,
                  <span class="author">Jiasen Lu</span>,
                  <span class="author">Hongsheng Li</span>,
                  <span class="author">Roozbeh Mottaghi</span>,
                  <span class="author">Aniruddha Kembhavi</span>,
                </div>
                <div>
                  <span class="venue">NeuIPS 2021</span> /
                  <span class="tag"><a href="https://arxiv.org/pdf/2106.01401.pdf">Paper</a></span> /
                  <span class="tag"><a href="https://github.com/gaopengcuhk/Container">Code</a></span>
                </div>
                <br>
              </div>
            </div>            
            
            
            <div class="publication">
              <div class="img"><img class="img_responsive" src="./imgs/smca.jpg" alt="autoassign"></div>
              <div class="text">
                <div class="title">Fast Convergence of DETR with Spatially Modulated Co-attention</div> 
                <div class="authors">
                  <span class="author jw">Peng Gao</span>,
                  <span class="author">Minghang Zeng</span>,
                  <span class="author">Xiaogang Wang</span>, 
                  <span class="author">Jifeng Dai</span>, 
                  <span class="author">Hongsheng Li</span>, 
                </div>
                <div>
                  <span class="venue">ICCV 2021</span> /
                  <span class="tag"><a href="https://arxiv.org/abs/2101.07448.pdf">Paper</a></span> / 
                  <span class="tag"><a href="https://github.com/gaopengcuhk/SMCA-DETR">Code</a></span>
                </div>
                <br>
              </div>
            </div>
            
            <div class="publication">
              <div class="img"><img class="img_responsive" src="./imgs/arxiv2021_nlp.jpg" alt="autoassign"></div>
              <div class="text">
                <div class="title">Scalable Transformers for Neural Machine Translation</div>
                <div class="authors">
                  <span class="author jw">Peng Gao</span>,
                  <span class="author">Shijie Geng</span>,
                  <span class="author">Yu Qiao</span>,
                  <span class="author">Xiaogang Wang</span>, 
                  <span class="author">Jifeng Dai</span>, 
                  <span class="author">Hongsheng Li</span>, 
                </div>
                <div>
                  <span class="venue">Arxiv 2021</span> /
                  <span class="tag"><a href="https://arxiv.org/pdf/2106.02242.pdf">Paper</a></span> /
                  <span class="tag"><a href="https://openreview.net/attachment?id=qfkz7wHXKC&name=supplementary_material">Code</a></span>
                </div>
                <br>
              </div>
            </div>
            
            <div class="publication">
              <div class="img"><img class="img_responsive" src="./imgs/dsnet.jpg" alt="siamrcr"></div>
              <div class="text">
                <div class="title">Dual Stream Network for Vision Recognition</div> 
                <div class="authors">
                  <span class="author">Mingyuan Mao*</span>,
                  <span class="author jw">Peng Gao*</span>,
                  <span class="author">Renrui Zhang*</span>,
                  <span class="author">Honghui Zheng*</span>, 
                  <span class="author">Teli Ma</span>, 
                  <span class="author">Yan Peng</span>, 
                  <span class="author">Errui Ding</span>,
                  <span class="author">Shumin Han</span>
                </div>
                <div>
                  <span class="venue">NeuIPS 2021</span> /
                  <span class="tag"><a href="https://arxiv.org/pdf/2105.14734.pdf">Paper</a></span>                   
                </div>
                <br>
              </div>
            </div>


            <div class="publication">
              <div class="img"><img class="img_responsive" src="./imgs/act.jpg" alt="autoassign"></div>
              <div class="text">
                <div class="title">End-to-End Object Detection with Adaptive Clustering Transformer</div>
                <div class="authors">
                  <span class="author">Minghang Zeng</span>,
                  <span class="author jw">Peng Gao</span>,
                  <span class="author">Renrui Zhang</span>,
                  <span class="author">Kunchang Li</span>,
                  <span class="author">Xiaogang Wang</span>,
                  <span class="author">Hongsheng Li</span>, 
                  <span class="author">Dong Hao</span>, 
                </div>
                <div>
                  <span class="venue">BMVC 2021 Oral</span> /
                  <span class="tag"><a href="https://arxiv.org/pdf/2011.09315.pdf">Paper</a></span> /
                  <span class="tag"><a href="https://github.com/gaopengcuhk/SMCA-DETR">Code</a></span>
                </div>
                <br>
              </div>
            </div>




            <div class="publication">
              <div class="img"><img class="img_responsive" src="./imgs/acmmm2021.jpg" alt="dynamichead"></div>
              <div class="text">
                <div class="title">Dense Contrastive Visual-Linguistic Pretraining</div> 
                <div class="authors">
                  <span class="author">Lei Shi</span>,
                  <span class="author">Kai Shuang</span>,
                  <span class="author">Shijie Geng</span>, 
                  <span class="author jw">Peng Gao</span>, 
                  <span class="author">Zuohui Fu</span>,
                  <span class="author">Gerard de Melo</span>,
                  <span class="author">Yunpeng Chen</span>
                  <span class="author">Sen Su</span>
                </div>
                <div>
                  <span class="venue">ACMMM 2021</span> /
                  <span class="tag"><a href="https://arxiv.org/pdf/2109.11778.pdf">Paper</a></span>
                </div>
                <br>
              </div>
            </div>


            <div class="publication">
              <div class="img"><img class="img_responsive" src="./imgs/aaai2021.jpg" alt="ltf-v2"></div>
              <div class="text">
                <div class="title">Dynamic Graph Representation Learning for Video Dialog via Multi-Modal Shuffled Transformers</div> 
                <div class="authors">
                  <span class="author">Shijie Geng</span>,
                  <span class="author jw">Peng Gao</span>,
                  <span class="author">Moitreya Chatterjee</span>, 
                  <span class="author">Chiori Hori</span>, 
                  <span class="author">Jonathan Le Roux</span>, 
                  <span class="author">Yongfeng Zhang</span>,
                  <span class="author">Hongsheng Li</span>,
                  <span class="author">Anoop Cherian</span>
                </div>
                <div>
                  <span class="venue">AAAI 2021</span> /
                  <span class="tag"><a href="https://arxiv.org/pdf/2007.03848.pdf">Paper</a></span>
                </div>
                <br>
              </div>
            </div>

            <div class="year" id="year"></div>
            <div class="text">
              <h3 style="margin-bottom:0em">
                2020
              </h3>
            </div>

            <div class="publication">
              <div class="img"><img class="img_responsive" src="./imgs/eccv2020.jpg" alt="lsts"></div>
              <div class="text">
                <div class="title">Learning Where to Focus for Efficient Video Object Detection</div> 
                <div class="authors">
                  <span class="author">Zhengkai Jiang</span>,
                  <span class="author">Yu Liu</span>,
                  <span class="author">Ceyuan Yang</span>,
                  <span class="author">Jihao Liu</span>,
                  <span class="author jw">Peng Gao</span>,
                  <span class="author">Qian Zhang</span>,
                  <span class="author">Shiming Xiang</span>
                  <span class="author">Chunhong Pan</span>
                </div>
                <div>
                  <span class="venue">ECCV 2020</span> /
                  <span class="tag"><a href="https://arxiv.org/pdf/1911.05253.pdf">Paper</a></span> / 
                  <span class="tag"><a href="https://github.com/jiangzhengkai/LSTS">Code</a></span> 
                </div>
                <br>
              </div>
            </div>

            <div class="year" id="year"></div>
            <div class="text">
              <h3 style="margin-bottom:0em">
                2019
              </h3>
            </div>

            <div class="publication">
              <div class="img"><img class="img_responsive" src="./imgs/iccv2019.jpg" alt="cbgs"></div>
              <div class="text">
                <div class="title">Multi-modality Latent Interaction Network for Visual Question Answering</div>
                <div class="authors">
                  <span class="author jw">Peng Gao</span>,
                  <span class="author">Haoxuan You</span>,
                  <span class="author">Zhanpeng Zhang</span>,
                  <span class="author">Xiaogang Wang</span>,
                  <span class="author">Hongsheng Li</span>
                </div>
                <div>
                  <span class="venue">ICCV 2019</span> /
                  <span class="tag"><a href="https://arxiv.org/abs/1908.04289.pdf">Paper</a></span>
                </div>
                <br>
              </div>
            </div>

            <div class="publication">
              <div class="img"><img class="img_responsive" src="./imgs/cvpr2019.jpg" alt="cross-modality"></div>
              <div class="text">
                <div class="title">Dynamic Fusion with Intra and Inter-Modality Attention Flow for Visual Question Answering</div>
                <div class="authors">
                  <span class="author jw">Peng Gao</span>,
                  <span class="author">Zhengkai Jiang</span>,
                  <span class="author">Haoxuan You</span>,
                  <span class="author">Pan Lu</span>,
                  <span class="author">Steven CH Hoi</span>,
                  <span class="author">Xiaogang Wang</span>,
                  <span class="author">Hongsheng Li</span>
                </div>
                <div>
                  <span class="venue">CVPR 2019</span> /
                  <span class="tag"><a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Gao_Dynamic_Fusion_With_Intra-_and_Inter-Modality_Attention_Flow_for_Visual_CVPR_2019_paper.html">Paper</a></span>
                </div>
                <div>
                  <span class="highlight">Oral Presentation</span>
                </div>
                <br>
              </div>
            </div>

            <div class="publication">
              <div class="img"><img class="img_responsive" src="./imgs/aaai2019.jpg" alt="lwdn"></div>
              <div class="text">
                <div class="title">Video Object Detection with Locally-Weightd Deformable Neighboors</div>
                <div class="authors">
                  <span class="author">Zhengkai Jiang</span>,
                  <span class="author jw">Peng Gao</span>,
                  <span class="author">Chaoxu Guo</span>,
                  <span class="author">Qian Zhang</span>,
                  <span class="author">Shiming Xiang</span>,
                  <span class="author">Chunhong Pan</span>
                </div>
                <div>
                  <span class="venue">AAAI 2019</span> /
                  <span class="tag"><a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4871">Paper</a></span>
                </div>
                <br>
              </div>
            </div>

            <div class="year" id="year"></div>
            <div class="text">
              <h3 style="margin-bottom:0em">
                2018
              </h3>
            </div>

            <div class="publication">
              <div class="img"><img class="img_responsive" src="./imgs/eccv2018.jpg" alt="lwdn"></div>
              <div class="text">
                <div class="title">Question-guided Hybrid Convolution for Visual Question Answering</div>
                <div class="authors">
                  <span class="author jw">Peng Gao</span>,
                  <span class="author">Hongsheng Li</span>,
                  <span class="author">Shuang Li</span>,
                  <span class="author">Pan Lu</span>,
                  <span class="author">Yikang Li</span>,
                  <span class="author">Steven C.H. Hoi</span>,
                  <span class="author">Xiaogang Wang</span>
                </div>
                <div>
                  <span class="venue">ECCV 2018</span> /
                  <span class="tag"><a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/gao_peng_Question-Guided_Hybrid_Convolution_ECCV_2018_paper.pdf">Paper</a></span>
                </div>
                <br>
              </div>
            </div>
       
          </ul>
      </div>  <!-- content -->
    </div> <!-- container -->
  </div> <!-- outer container -->
<script>showPubs(0);</script>
<script>var scroll = new SmoothScroll('a[href*="#"]', {speed: 1000});</script>
</div><script>mendeleyWebImporter = { open: function () { window.postMessage('0.6076674848300223', 'https://gaopengpjlab.github.io') } }</script><youdao_grammar_result><div></div></youdao_grammar_result><youdao-grammar-editor><div></div></youdao-grammar-editor>
</div>
</body>
</html>
